import streamlit as st
import io
import re
import zipfile
import math
import chardet

# NLP Imports
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk import pos_tag

# DOCX Import
from docx import Document

# =========== [æ–°å¢] å¼•å…¥ OpenAI åº“ ===========
from openai import OpenAI
# ============================================

# Optional Spacy
try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

# ------------------ é¡µé¢é…ç½® ------------------
st.set_page_config(page_title="å•è¯æå–å™¨ & AI åŠ©æ•™", page_icon="ğŸ“˜", layout="wide")

# ------------------ ç¼“å­˜èµ„æºåŠ è½½ ------------------
@st.cache_resource
def download_nltk_resources():
    """é™é»˜ä¸‹è½½ NLTK èµ„æº"""
    resources = ["punkt", "averaged_perceptron_tagger", "wordnet", "omw-1.4", "stopwords"]
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            nltk.download(r, quiet=True)
        except ValueError:
            nltk.download(r, quiet=True)

@st.cache_resource
def load_spacy_model():
    if _HAS_SPACY:
        try:
            return spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception:
            return None
    return None

# åˆå§‹åŒ–èµ„æº
download_nltk_resources()
nlp_spacy = load_spacy_model()

# ------------------ æ ¸å¿ƒå·¥å…·å‡½æ•° (ä¿æŒåŸæ ·) ------------------

def extract_text_from_bytes(file_obj, filename):
    """ä»å†…å­˜æ–‡ä»¶å¯¹è±¡ä¸­æå–æ–‡æœ¬"""
    ext = filename.split('.')[-1].lower()
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_obj)
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            text = "\n".join(paragraphs)
        else:
            raw = file_obj.read()
            enc = chardet.detect(raw).get('encoding') or 'utf-8'
            text = raw.decode(enc, errors='ignore')
    except Exception as e:
        st.warning(f"âš ï¸ è¯»å– {filename} å¤±è´¥: {e}")
        return ""

    if ext == 'srt': return extract_english_from_srt(text)
    elif ext == 'ass': return extract_english_from_ass(text)
    elif ext == 'vtt': return extract_english_from_vtt(text)
    else: return text

def extract_english_from_srt(text):
    lines = []
    SRT_TIME_RE = re.compile(r"^\d{2}:\d{2}:\d{2}[,.]\d{3}")
    for ln in text.splitlines():
        s = ln.strip()
        if not s or s.isdigit() or SRT_TIME_RE.match(s): continue
        s = re.sub(r"<.*?>", "", s)
        s = re.sub(r"\[.*?\]", "", s)
        parts = re.findall(r"[A-Za-z0-9'\",.?!:;()\- ]+", s)
        if parts: lines.append("".join(parts).strip())
    return " ".join(lines)

def extract_english_from_ass(text):
    lines = []
    for ln in text.splitlines():
        if ln.startswith("Dialogue:"):
            parts = ln.split(",", 9)
            if len(parts) >= 10:
                t = re.sub(r"\{.*?\}", "", parts[-1])
                t = re.sub(r"<.*?>", "", t)
                parts2 = re.findall(r"[A-Za-z0-9'\",.?!:;()\- ]+", t)
                if parts2: lines.append("".join(parts2).strip())
    return " ".join(lines)

def extract_english_from_vtt(text):
    lines = []
    VTT_TIME_RE = re.compile(r"^\d{2}:\d{2}:\d{2}\.\d{3}")
    for ln in text.splitlines():
        s = ln.strip()
        if not s or s.startswith("WEBVTT") or VTT_TIME_RE.match(s): continue
        s = re.sub(r"<.*?>", "", s)
        parts = re.findall(r"[A-Za-z0-9'\",.?!:;()\- ]+", s)
        if parts: lines.append("".join(parts).strip())
    return " ".join(lines)

def get_wordnet_pos(tag):
    if tag.startswith('J'): return wordnet.ADJ
    if tag.startswith('V'): return wordnet.VERB
    if tag.startswith('N'): return wordnet.NOUN
    if tag.startswith('R'): return wordnet.ADV
    return None

def process_words(all_text, mode, min_len, filter_set=None):
    """å¤„ç†æ ¸å¿ƒé€»è¾‘"""
    TOKEN_RE = re.compile(r"[A-Za-z-]+")
    raw_tokens = TOKEN_RE.findall(all_text)
    cleaned = [re.sub(r'[^a-z]', '', w.lower()) for w in raw_tokens]
    cleaned = [w for w in cleaned if w]

    lemmatized = []
    progress_bar = st.progress(0)
    status_text = st.empty()

    if mode == "spacy" and nlp_spacy is not None:
        status_text.text("æ­£åœ¨ä½¿ç”¨ spaCy (ç²¾å‡†æ¨¡å¼)...")
        chunk_size = 50000
        chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]
        for i, chunk in enumerate(chunks):
            doc = nlp_spacy(" ".join(chunk))
            for token in doc:
                lw = token.lemma_.lower()
                if lw.isalpha() and wordnet.synsets(lw):
                    lemmatized.append(lw)
            progress_bar.progress((i + 1) / len(chunks))
    else:
        status_text.text("æ­£åœ¨ä½¿ç”¨ NLTK (å¿«é€Ÿæ¨¡å¼)...")
        lemmatizer = WordNetLemmatizer()
        tagged = pos_tag(cleaned)
        total = len(tagged)
        for i, (w, tag) in enumerate(tagged):
            wn = get_wordnet_pos(tag)
            lw = lemmatizer.lemmatize(w, wn) if wn else lemmatizer.lemmatize(w)
            if wordnet.synsets(lw):
                lemmatized.append(lw)
            if i % 5000 == 0:
                progress_bar.progress(min(i / total, 1.0))
        progress_bar.progress(1.0)

    status_text.text("æ­£åœ¨å»é‡å’Œè¿‡æ»¤...")
    seen = set()
    final_words = []
    sys_stopwords = set(stopwords.words('english'))
    
    for w in lemmatized:
        if len(w) < min_len: continue
        if w in sys_stopwords: continue
        if filter_set and w in filter_set: continue
        if w not in seen:
            seen.add(w)
            final_words.append(w)
            
    status_text.empty()
    progress_bar.empty()
    return final_words

# ------------------ UI å¸ƒå±€ ------------------

st.title("ğŸ“˜ è‹±è¯­ç”Ÿè¯æœ¬ & AI åŠ©æ•™")
st.markdown("ä¸Šä¼ å­—å¹•/æ–‡æ¡£ -> æå–å•è¯ -> **AI è¾…åŠ©å­¦ä¹ **")

with st.sidebar:
    st.header("âš™ï¸ æå–è®¾ç½®")
    nlp_mode = st.selectbox("NLP å¼•æ“", ["nltk (å¿«é€Ÿ)", "spacy (ç²¾å‡†)"], index=0)
    mode_key = "spacy" if "spacy" in nlp_mode else "nltk"
    min_len = st.number_input("æœ€å°é•¿åº¦", min_value=1, value=3)
    chunk_size = st.number_input("åˆ‡åˆ†å¤§å°", min_value=100, value=5000)
    sort_order = st.radio("æ’åº", ["æŒ‰å‡ºç°é¡ºåº", "A-Z æ’åº", "éšæœºæ‰“ä¹±"])
    
    st.divider()
    filter_file = st.file_uploader("è¿‡æ»¤è¯è¡¨ (.txt)", type=['txt'])
    filter_set = set()
    if filter_file:
        content = filter_file.getvalue().decode("utf-8", errors='ignore')
        filter_set = set(line.strip().lower() for line in content.splitlines() if line.strip())
        st.success(f"å·²åŠ è½½ {len(filter_set)} ä¸ªè¿‡æ»¤è¯")

    # =========== [æ–°å¢] Agent è®¾ç½® ===========
    st.divider()
    st.header("ğŸ¤– AI è®¾ç½®")
    # ä¼˜å…ˆä» Secrets è¯»å– API Keyï¼Œæ–¹ä¾¿éƒ¨ç½²åä¸ç”¨æ¯æ¬¡è¾“å…¥
    if "OPENAI_API_KEY" in st.secrets:
        api_key = st.secrets["OPENAI_API_KEY"]
        st.success("å·²æ£€æµ‹åˆ°é…ç½®çš„ API Key")
    else:
        api_key = st.text_input("API Key", type="password", help="è¾“å…¥ OpenAI/DeepSeek Key")
        
    base_url = st.text_input("API URL", value="https://api.deepseek.com", help="ä¾‹å¦‚ DeepSeek æˆ– OpenAI åœ°å€")
    model_name = st.text_input("æ¨¡å‹åç§°", value="deepseek-chat")
    # ========================================

# =========== [æ–°å¢] åˆå§‹åŒ–è®°å¿† (Session State) ===========
if "agent_memory" not in st.session_state:
    st.session_state.agent_memory = None
if "messages" not in st.session_state:
    st.session_state.messages = []
# =======================================================

uploaded_files = st.file_uploader("æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„", type=['txt', 'srt', 'ass', 'vtt', 'docx'], accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸš€ å¼€å§‹æå–", type="primary"):
        all_raw_text = []
        read_bar = st.progress(0)
        
        for i, file in enumerate(uploaded_files):
            file.seek(0)
            text = extract_text_from_bytes(file, file.name)
            all_raw_text.append(text)
            read_bar.progress((i + 1) / len(uploaded_files))
        read_bar.empty()
        
        full_text = "\n".join(all_raw_text)
        result_words = process_words(full_text, mode_key, min_len, filter_set)
        
        if sort_order == "A-Z æ’åº":
            result_words.sort()
        elif sort_order == "éšæœºæ‰“ä¹±":
            import random
            random.shuffle(result_words)
            
        st.success(f"ğŸ‰ æå–å®Œæˆï¼å…± {len(result_words)} ä¸ªå•è¯ã€‚")
        
        # ç»“æœä¸‹è½½
        if result_words:
            zip_buffer = io.BytesIO()
            num_files = math.ceil(len(result_words) / chunk_size)
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for i in range(num_files):
                    start = i * chunk_size
                    end = min(start + chunk_size, len(result_words))
                    fname = f"word_list_{i+1}.txt"
                    zf.writestr(fname, "\n".join(result_words[start:end]))
            
            st.download_button("ğŸ“¥ ä¸‹è½½ç”Ÿè¯æœ¬ (ZIP)", zip_buffer.getvalue(), "words.zip", "application/zip")
            
            # =========== [æ–°å¢] æ³¨å…¥è®°å¿† ===========
            st.session_state.agent_memory = {
                "text": full_text,
                "words": result_words
            }
            st.session_state.messages = [] # æ¸…ç©ºæ—§å¯¹è¯
            st.toast("AI å·²å‡†å¤‡å¥½ï¼Œè¯·ä¸‹æ»‘æŸ¥çœ‹ï¼", icon="ğŸ¤–")
            # ======================================
        else:
            st.warning("æœªæå–åˆ°å•è¯ã€‚")

# =========== [æ–°å¢] AI èŠå¤©äº¤äº’åŒº ===========
st.divider()

if st.session_state.agent_memory:
    st.subheader("ğŸ¤– AI åŠ©æ•™")
    st.caption("åŸºäºä½ ä¸Šä¼ çš„æ–‡æ¡£å†…å®¹å›ç­”")
    
    # å¿«æ·æŒ‰é’®
    col1, col2 = st.columns(2)
    if col1.button("ğŸ“ ç”¨å‰10ä¸ªç”Ÿè¯å†™æ•…äº‹"):
        st.session_state.messages.append({"role": "user", "content": "è¯·ç”¨æå–å‡ºçš„å‰10ä¸ªå•è¯å†™ä¸€ä¸ªçŸ­ç¯‡è‹±æ–‡æ•…äº‹ï¼Œå¹¶é™„å¸¦ä¸­æ–‡ç¿»è¯‘ï¼Œç”Ÿè¯è¯·åŠ ç²—ã€‚"})
    if col2.button("ğŸ§ å‡º3é“é˜…è¯»ç†è§£é¢˜"):
        st.session_state.messages.append({"role": "user", "content": "æ ¹æ®åŸæ–‡å†…å®¹å‡º3é“å•é¡¹é€‰æ‹©é¢˜ï¼Œå¹¶åœ¨æœ€åé™„ä¸Šç­”æ¡ˆå’Œè§£æã€‚"})

    # æ˜¾ç¤ºå†å²
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # è¾“å…¥æ¡†
    if prompt := st.chat_input("ä¾‹å¦‚ï¼š'distinguish' åœ¨æ–‡ä¸­æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ"):
        if not api_key:
            st.error("è¯·å…ˆåœ¨å·¦ä¾§è®¾ç½® API Key")
            st.stop()
            
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # æ„å»º Prompt (RAG)
        mem = st.session_state.agent_memory
        # ä¸ºäº†é˜²æ­¢ Token è¶…é™ï¼Œåªæˆªå–å‰ 3000 å­—ç¬¦å’Œå‰ 100 ä¸ªå•è¯ä½œä¸ºä¸Šä¸‹æ–‡
        context_text = mem["text"][:3000] + "..."
        context_words = ", ".join(mem["words"][:100])
        
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªè‹±è¯­åŠ©æ•™ã€‚
        ã€èµ„æ–™ã€‘ï¼š
        1. æ ¸å¿ƒç”Ÿè¯ï¼š{context_words}
        2. åŸæ–‡ç‰‡æ®µï¼š{context_text}
        
        ã€ä»»åŠ¡ã€‘ï¼š
        åŸºäºèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è‹¥è¯¢é—®å•è¯å«ä¹‰ï¼Œè¯·ç»“åˆåŸæ–‡è¯­å¢ƒè§£é‡Šã€‚
        """
        
        try:
            client = OpenAI(api_key=api_key, base_url=base_url)
            with st.chat_message("assistant"):
                stream = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        *st.session_state.messages
                    ],
                    stream=True
                )
                response = st.write_stream(stream)
            st.session_state.messages.append({"role": "assistant", "content": response})
        except Exception as e:
            st.error(f"API é”™è¯¯: {e}")

elif uploaded_files:
    st.info("ğŸ‘† æå–å®Œæˆåï¼Œè¿™é‡Œä¼šå‡ºç° AI èŠå¤©ç•Œé¢ã€‚")
